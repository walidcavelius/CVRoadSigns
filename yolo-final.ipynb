{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Import Libraries**"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T17:58:50.246212Z","iopub.status.busy":"2024-06-23T17:58:50.245564Z","iopub.status.idle":"2024-06-23T17:58:50.256811Z","shell.execute_reply":"2024-06-23T17:58:50.255815Z","shell.execute_reply.started":"2024-06-23T17:58:50.246177Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x79d0e92d3430>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","import os, csv\n","import PIL\n","import skimage\n","from skimage import io\n","import numpy as np\n","from PIL import Image, ImageDraw\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import torchvision.transforms as transforms\n","from torchvision.tv_tensors import BoundingBoxes, Mask\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","import torch.optim as optim\n","import torchvision.transforms.functional as FT\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","import cv2\n","from collections import Counter\n","\n","torch.manual_seed(1337)"]},{"cell_type":"markdown","metadata":{},"source":["# **Model Architecture**"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T17:58:50.268455Z","iopub.status.busy":"2024-06-23T17:58:50.268186Z","iopub.status.idle":"2024-06-23T17:58:50.279199Z","shell.execute_reply":"2024-06-23T17:58:50.278292Z","shell.execute_reply.started":"2024-06-23T17:58:50.268431Z"},"trusted":true},"outputs":[],"source":["architecture_config = [\n","    #Tuple: (kernel_size, number of filters, strides, padding)\n","    (7, 64, 2, 3),\n","    #\"M\" = Max Pool Layer\n","    \"M\",\n","    (3, 192, 1, 1),\n","    \"M\",\n","    (1, 128, 1, 0),\n","    (3, 256, 1, 1),\n","    (1, 256, 1, 0),\n","    (3, 512, 1, 1),\n","    \"M\",\n","    #List: [(tuple), (tuple), how many times to repeat]\n","    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n","    (1, 512, 1, 0),\n","    (3, 1024, 1, 1),\n","    \"M\",\n","    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n","    (3, 1024, 1, 1),\n","    (3, 1024, 2, 1),\n","    (3, 1024, 1, 1),\n","    (3, 1024, 1, 1),\n","    #Doesnt include fc layers\n","]\n","\n","class Config:\n","    S = 12\n","    C = 7\n","    B = 2\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T17:58:50.281178Z","iopub.status.busy":"2024-06-23T17:58:50.280843Z","iopub.status.idle":"2024-06-23T17:58:50.656470Z","shell.execute_reply":"2024-06-23T17:58:50.655542Z","shell.execute_reply.started":"2024-06-23T17:58:50.281141Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of parameters: 34774976\n"]}],"source":["class CNNBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, **kwargs):\n","        super(CNNBlock, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n","        self.batchnorm = nn.BatchNorm2d(out_channels)\n","        self.leakyrelu = nn.LeakyReLU(0.1)\n","        \n","    def forward(self, x):\n","        return self.leakyrelu(self.batchnorm(self.conv(x)))\n","    \n","class YoloV1(nn.Module):\n","    def __init__(self, in_channels=3, **kwargs):\n","        super(YoloV1, self).__init__()\n","        self.architecture = architecture_config\n","        self.in_channels = in_channels\n","        self.darknet = self._create_conv_layers(self.architecture)\n","        self.fcs = self._create_fcs(**kwargs)\n","        \n","    def forward(self, x):\n","        x = self.darknet(x)\n","        return self.fcs(torch.flatten(x, start_dim=1))\n","    \n","    def _create_conv_layers(self, architecture):\n","        layers = []\n","        in_channels = self.in_channels\n","        \n","        for x in architecture:\n","            if type(x) == tuple:\n","                layers += [CNNBlock(in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3])]\n","                in_channels = x[1]\n","            elif type(x) == str:\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            elif type(x) == list:\n","                conv1 = x[0] #Tuple\n","                conv2 = x[1] #Tuple\n","                repeats = x[2] #Int\n","                \n","                for _ in range(repeats):\n","                    layers += [CNNBlock(in_channels, conv1[1], kernel_size=conv1[0], stride=conv1[2], padding=conv1[3])]\n","                    layers += [CNNBlock(conv1[1], conv2[1], kernel_size=conv2[0], stride=conv2[2], padding=conv2[3])]\n","                    in_channels = conv2[1]\n","                    \n","        return nn.Sequential(*layers)\n","    \n","    def _create_fcs(self):\n","        S, B, C = Config.S, Config.B, Config.C\n","        # original code 496 (instead of currently 4096); but they mentioned that 4096 was used in the paper\n","        \n","        return nn.Sequential(nn.Flatten(), nn.Linear(288 * S * S, 496), nn.Dropout(0.0), nn.LeakyReLU(0.1), nn.Linear(496, S * S * (C + B * 5)))#Original paper uses nn.Linear(1024 * S * S, 4096) not 496. Also the last layer will be reshaped to (S, S, 13) where C+B*5 = 13\n","    \n","model = YoloV1()#.to(DEVICE)\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"Number of parameters: {total_params}\")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T17:58:50.658752Z","iopub.status.busy":"2024-06-23T17:58:50.658435Z","iopub.status.idle":"2024-06-23T17:58:50.670844Z","shell.execute_reply":"2024-06-23T17:58:50.669827Z","shell.execute_reply.started":"2024-06-23T17:58:50.658725Z"},"trusted":true},"outputs":[],"source":["def intersection_over_union(boxes_preds, boxes_labels, box_format='midpoint'):\n","    \"\"\"\n","    Parameters:\n","        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n","        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n","        box_format (str): midpoint/corners, if boxes are (x,y,w,h) or (x1,y1,x2,y2) respectively.\n","    \n","    Returns:\n","        tensor: Intersection over union for all examples\n","    \"\"\"\n","    \n","    if box_format == 'midpoint':\n","        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n","        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n","        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n","        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n","        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n","        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n","        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n","        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n","        \n","    if box_format == 'corners':\n","        box1_x1 = boxes_preds[..., 0:1]\n","        box1_y1 = boxes_preds[..., 1:2]\n","        box1_x2 = boxes_preds[..., 2:3]\n","        box1_y2 = boxes_preds[..., 3:4] # Output tensor should be (N, 1). If we only use 3, we go to (N)\n","        box2_x1 = boxes_labels[..., 0:1]\n","        box2_y1 = boxes_labels[..., 1:2]\n","        box2_x2 = boxes_labels[..., 2:3]\n","        box2_y2 = boxes_labels[..., 3:4]\n","    \n","    x1 = torch.max(box1_x1, box2_x1)\n","    y1 = torch.max(box1_y1, box2_y1)\n","    x2 = torch.min(box1_x2, box2_x2)\n","    y2 = torch.min(box1_y2, box2_y2)\n","    \n","    #.clamp(0) is for the case when they don't intersect. Since when they don't intersect, one of these will be negative so that should become 0\n","    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n","    \n","    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n","    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n","    \n","    return intersection / (box1_area + box2_area - intersection + 1e-6)"]},{"cell_type":"markdown","metadata":{},"source":["# **Non-Max Supression**"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T17:58:50.673093Z","iopub.status.busy":"2024-06-23T17:58:50.672279Z","iopub.status.idle":"2024-06-23T17:58:50.684830Z","shell.execute_reply":"2024-06-23T17:58:50.684030Z","shell.execute_reply.started":"2024-06-23T17:58:50.673032Z"},"trusted":true},"outputs":[],"source":["def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n","    \"\"\"\n","    Parameters:\n","        bboxes (list): list of lists containing all bboxes with each bboxes\n","        specified as [class_pred, prob_score, x1, y1, x2, y2]\n","        iou_threshold (float): threshold where predicted bboxes is correct\n","        threshold (float): threshold to remove predicted bboxes (independent of IoU) \n","        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n","    Returns:\n","        list: bboxes after performing NMS given a specific IoU threshold\n","    \"\"\"\n","\n","    assert type(bboxes) == list\n","\n","    bboxes = [box for box in bboxes if box[1] > threshold]\n","    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n","    bboxes_after_nms = []\n","\n","    while bboxes:\n","        chosen_box = bboxes.pop(0)\n","\n","        bboxes = [\n","            box\n","            for box in bboxes\n","            if box[0] != chosen_box[0]\n","            or intersection_over_union(\n","                torch.tensor(chosen_box[2:]),\n","                torch.tensor(box[2:]),\n","                box_format=box_format,\n","            )\n","            < iou_threshold\n","        ]\n","\n","        bboxes_after_nms.append(chosen_box)\n","\n","    return bboxes_after_nms"]},{"cell_type":"markdown","metadata":{},"source":["# **Mean Average Precision**"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T17:58:50.687474Z","iopub.status.busy":"2024-06-23T17:58:50.687142Z","iopub.status.idle":"2024-06-23T17:58:50.703870Z","shell.execute_reply":"2024-06-23T17:58:50.702881Z","shell.execute_reply.started":"2024-06-23T17:58:50.687447Z"},"trusted":true},"outputs":[],"source":["def mean_average_precision(\n","    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n","):\n","    \"\"\"\n","    Parameters:\n","        pred_boxes (list): list of lists containing all bboxes with each bboxes\n","        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n","        true_boxes (list): Similar as pred_boxes except all the correct ones \n","        iou_threshold (float): threshold where predicted bboxes is correct\n","        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n","        num_classes (int): number of classes\n","    Returns:\n","        float: mAP value across all classes given a specific IoU threshold \n","    \"\"\"\n","\n","    # list storing all AP for respective classes\n","    average_precisions = []\n","\n","    # used for numerical stability later on\n","    epsilon = 1e-6\n","\n","    for c in range(Config.C):\n","        detections = []\n","        ground_truths = []\n","\n","        # Go through all predictions and targets,\n","        # and only add the ones that belong to the\n","        # current class c\n","        for detection in pred_boxes:\n","            if detection[1] == c:\n","                detections.append(detection)\n","\n","        for true_box in true_boxes:\n","            if true_box[1] == c:\n","                ground_truths.append(true_box)\n","\n","        # find the amount of bboxes for each training example\n","        # Counter here finds how many ground truth bboxes we get\n","        # for each training example, so let's say img 0 has 3,\n","        # img 1 has 5 then we will obtain a dictionary with:\n","        # amount_bboxes = {0:3, 1:5}\n","        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n","\n","        # We then go through each key, val in this dictionary\n","        # and convert to the following (w.r.t same example):\n","        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n","        for key, val in amount_bboxes.items():\n","            amount_bboxes[key] = torch.zeros(val)\n","\n","        # sort by box probabilities which is index 2\n","        detections.sort(key=lambda x: x[2], reverse=True)\n","        TP = torch.zeros((len(detections)))\n","        FP = torch.zeros((len(detections)))\n","        total_true_bboxes = len(ground_truths)\n","        \n","        # If none exists for this class then we can safely skip\n","        if total_true_bboxes == 0:\n","            continue\n","\n","        for detection_idx, detection in enumerate(detections):\n","            # Only take out the ground_truths that have the same\n","            # training idx as detection\n","            ground_truth_img = [\n","                bbox for bbox in ground_truths if bbox[0] == detection[0]\n","            ]\n","\n","            num_gts = len(ground_truth_img)\n","            best_iou = 0\n","\n","            for idx, gt in enumerate(ground_truth_img):\n","                iou = intersection_over_union(\n","                    torch.tensor(detection[3:]),\n","                    torch.tensor(gt[3:]),\n","                    box_format=box_format,\n","                )\n","\n","                if iou > best_iou:\n","                    best_iou = iou\n","                    best_gt_idx = idx\n","\n","            if best_iou > iou_threshold:\n","                # only detect ground truth detection once\n","                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n","                    # true positive and add this bounding box to seen\n","                    TP[detection_idx] = 1\n","                    amount_bboxes[detection[0]][best_gt_idx] = 1\n","                else:\n","                    FP[detection_idx] = 1\n","\n","            # if IOU is lower then the detection is a false positive\n","            else:\n","                FP[detection_idx] = 1\n","\n","        TP_cumsum = torch.cumsum(TP, dim=0)\n","        FP_cumsum = torch.cumsum(FP, dim=0)\n","        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n","        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n","        precisions = torch.cat((torch.tensor([1]), precisions))\n","        recalls = torch.cat((torch.tensor([0]), recalls))\n","        # torch.trapz for numerical integration\n","        average_precisions.append(torch.trapz(precisions, recalls))\n","\n","    return sum(average_precisions) / len(average_precisions)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T17:58:50.705353Z","iopub.status.busy":"2024-06-23T17:58:50.704992Z","iopub.status.idle":"2024-06-23T17:58:50.727962Z","shell.execute_reply":"2024-06-23T17:58:50.726842Z","shell.execute_reply.started":"2024-06-23T17:58:50.705326Z"},"trusted":true},"outputs":[],"source":["def get_bboxes(loader, model, iou_threshold, threshold, pred_format=\"cells\", \n","               box_format=\"midpoint\", device=\"cuda\", output_x = False):\n","    all_pred_boxes = []\n","    all_true_boxes = []\n","\n","    model.eval()\n","    train_idx = 0\n","    \n","    if output_x:\n","        x_out = []\n","\n","    for batch_idx, (x, labels) in enumerate(loader):\n","        x = x.to(device)\n","        labels = labels.to(device)\n","\n","        with torch.no_grad():\n","            predictions = model(x)\n","\n","        batch_size = x.shape[0]\n","        true_bboxes = cellboxes_to_boxes(labels)\n","        bboxes = cellboxes_to_boxes(predictions)\n","\n","        for idx in range(batch_size):\n","            \n","            nms_boxes = non_max_suppression(\n","                bboxes[idx],\n","                iou_threshold=iou_threshold,\n","                threshold=threshold,\n","                box_format=box_format,\n","            )\n","\n","            for nms_box in nms_boxes:\n","                all_pred_boxes.append([train_idx] + nms_box)\n","                \n","                if output_x:\n","                    x_out.append([train_idx, x[idx], labels[idx], nms_box])\n","\n","            for box in true_bboxes[idx]:\n","                if box[1] > threshold:\n","                    all_true_boxes.append([train_idx] + box)\n","\n","            train_idx += 1\n","\n","    model.train()\n","    if output_x:\n","        return all_pred_boxes, all_true_boxes, x_out\n","    return all_pred_boxes, all_true_boxes\n","\n","def convert_cellboxes(predictions):\n","    S = Config.S\n","    C = Config.C\n","    \"\"\"\n","    Converts bounding boxes output from Yolo with\n","    an image split size of S into entire image ratios\n","    rather than relative to cell ratios. Tried to do this\n","    vectorized, but this resulted in quite difficult to read\n","    code... Use as a black box? Or implement a more intuitive,\n","    using 2 for loops iterating range(S) and convert them one\n","    by one, resulting in a slower but more readable implementation.\n","    \"\"\"\n","\n","    predictions = predictions.to(\"cpu\")\n","    batch_size = predictions.shape[0]\n","    predictions = predictions.reshape(batch_size, Config.S, Config.S, C + 10)\n","    bboxes1 = predictions[..., C + 1:C + 5]\n","    bboxes2 = predictions[..., C + 6:C + 10]\n","    scores = torch.cat(\n","        (predictions[..., C].unsqueeze(0), predictions[..., C + 5].unsqueeze(0)), dim=0\n","    )\n","    best_box = scores.argmax(0).unsqueeze(-1)\n","    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n","    cell_indices = torch.arange(Config.S).repeat(batch_size, Config.S, 1).unsqueeze(-1)\n","    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n","    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n","    w_y = 1 / S * best_boxes[..., 2:4]\n","    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n","    predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n","    best_confidence = torch.max(predictions[..., C], predictions[..., C + 5]).unsqueeze(\n","        -1\n","    )\n","    converted_preds = torch.cat(\n","        (predicted_class, best_confidence, converted_bboxes), dim=-1\n","    )\n","\n","    return converted_preds\n","\n","\n","def cellboxes_to_boxes(out):\n","    S = Config.S\n","    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n","    converted_pred[..., 0] = converted_pred[..., 0].long()\n","    all_bboxes = []\n","\n","    for ex_idx in range(out.shape[0]):\n","        bboxes = []\n","\n","        for bbox_idx in range(S * S):\n","            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n","        all_bboxes.append(bboxes)\n","\n","    return all_bboxes\n","\n","def save_checkpoint(state, filename=\"my_checkpoint.pth\"):\n","    print(\"=> Saving checkpoint\")\n","    torch.save(state, filename)\n","    \n","def load_checkpoint(checkpoint, model, optimizer):\n","    print(\"=> Loading checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])"]},{"cell_type":"markdown","metadata":{},"source":["# **Dataset Preprocessing**"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T17:58:50.729890Z","iopub.status.busy":"2024-06-23T17:58:50.729538Z","iopub.status.idle":"2024-06-23T17:58:50.752210Z","shell.execute_reply":"2024-06-23T17:58:50.751280Z","shell.execute_reply.started":"2024-06-23T17:58:50.729860Z"},"trusted":true},"outputs":[],"source":["files_dir = '../input/sy32-panneaux-2/train'\n","val_dir = '../input/sy32-panneaux-2/val'\n","test_dir = '../input/sy32-panneaux-2/test'\n","\n","excluded = ['0716', '0265', '0546']\n","\n","images = [image for image in sorted(os.listdir(files_dir+'/images'))\n","                        if image[-4:]=='.jpg' and not image[-8:-4] in excluded]\n","annots = []\n","for image in images:\n","    annot = image.replace('images/','labels/').replace('.jpg','.csv')\n","    annots.append(annot)\n","    \n","images = pd.Series(images, name='images')\n","annots = pd.Series(annots, name='annots')\n","df = pd.concat([images, annots], axis=1)\n","df = pd.DataFrame(df)\n","\n","test_images = [image for image in sorted(os.listdir(test_dir))\n","                        if image[-4:]=='.jpg']\n","\n","test_annots = []\n","for image in test_images:\n","    test_annots.append(None)\n","\n","test_images = pd.Series(test_images, name='test_images')\n","test_annots = pd.Series(test_annots, name='test_annots')\n","test_df = pd.concat([test_images, test_annots], axis=1)\n","test_df = pd.DataFrame(test_df)\n","\n","##############\n","\n","val_images = [image for image in sorted(os.listdir(val_dir+'/images'))\n","                        if image[-4:]=='.jpg']\n","\n","val_annots = []\n","for image in val_images:\n","    annot = image.replace('images/','labels/').replace('.jpg','.csv')\n","    val_annots.append(annot)\n","\n","val_images = pd.Series(val_images, name='val_images')\n","val_annots = pd.Series(val_annots, name='val_images')\n","val_df = pd.concat([val_images, val_annots], axis=1)\n","val_df = pd.DataFrame(val_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T17:58:50.753836Z","iopub.status.busy":"2024-06-23T17:58:50.753466Z","iopub.status.idle":"2024-06-23T17:58:50.762425Z","shell.execute_reply":"2024-06-23T17:58:50.761404Z","shell.execute_reply.started":"2024-06-23T17:58:50.753801Z"},"trusted":true},"outputs":[],"source":["class_dictionary = ['danger', 'interdiction', 'stop', 'ceder', 'frouge', 'forange', 'fvert']\n","    \n","new_transform = A.Compose([\n","    A.Rotate(limit=15, p=0.6, border_mode=cv2.BORDER_CONSTANT),\n","    A.HorizontalFlip(p=0.3),\n","    A.RandomCropFromBorders(crop_left=0.15, crop_right=0.15, crop_top=0.15, crop_bottom=0.15, p=0.5),\n","    A.Resize(576, 576),\n","    ToTensorV2(), \n","    \n","], bbox_params=A.BboxParams(format='yolo'))\n","\n","    \n","new_transform_val = A.Compose([\n","    A.Resize(576, 576),\n","    #A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225))\n","    ToTensorV2(),\n","    \n","], bbox_params=A.BboxParams(format='yolo'))\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T17:58:50.763793Z","iopub.status.busy":"2024-06-23T17:58:50.763519Z","iopub.status.idle":"2024-06-23T17:58:50.777608Z","shell.execute_reply":"2024-06-23T17:58:50.776611Z","shell.execute_reply.started":"2024-06-23T17:58:50.763769Z"},"trusted":true},"outputs":[],"source":["class TrafficSignTestImagesDataset(torch.utils.data.Dataset):\n","    def __init__(self, df, files_dir, transform=None):\n","        self.annotations = df\n","        self.files_dir = files_dir\n","        self.transform = transform\n","        self.S = Config.S\n","        self.B = Config.B\n","        self.C = Config.C\n","        \n","    def __len__(self):\n","        return len(self.annotations) \n","    \n","    def __getitem__(self, index):\n","        dataset_row = self.annotations.iloc[index]\n","        image_path = os.path.join(self.files_dir, dataset_row[0])\n","        #label_path = os.path.join(self.files_dir, 'labels', dataset_row[1])\n","        \n","        boxes = []\n","        labels = []\n","        \n","        img = Image.open(image_path)\n","        img_width, img_height = img.size\n","        \n","        image = img.convert(\"RGB\")\n","        image = np.array(image)\n","        \n","        if self.transform:\n","            transformed = self.transform(image=image, bboxes=[])\n","            image = transformed['image']\n","            \n","        image = torch.tensor(image, dtype=torch.float32) / 255.0\n","        \n","        y = [dataset_row[0], img_width, img_height]\n","        \n","        return image, y\n","    \n","  "]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T17:58:50.779374Z","iopub.status.busy":"2024-06-23T17:58:50.779074Z","iopub.status.idle":"2024-06-23T17:58:50.807937Z","shell.execute_reply":"2024-06-23T17:58:50.807031Z","shell.execute_reply.started":"2024-06-23T17:58:50.779349Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'\\n\\nfor ridx, ax in enumerate(axs.ravel()):\\n    idx = ridx + 560\\n    im = torch.moveaxis(train_dataset[idx][0], 0, 2).cpu().numpy()\\n    \\n    image = Image.fromarray((im * 255).astype(np.uint8))\\n    draw = ImageDraw.Draw(image)\\n    \\n    \\n    labels = train_dataset[idx][1]\\n    S_SIZE = im.shape[1] / Config.S\\n    \\n    for i in range(Config.S):\\n        for j in range(Config.S):\\n            if labels[i, j, Config.C] == 1.0:\\n                [x, y, lwidth, lheight] = labels[i, j, 8:12] * S_SIZE\\n                lwidth /= 2\\n                lheight /= 2\\n                \\n                x += j * S_SIZE\\n                y += i * S_SIZE\\n                \\n                xy = [x - lwidth, y - lheight, x + lwidth, y + lheight]\\n        \\n                label = int( np.argmax(labels[i, j, :7]))\\n                print(\"label\", class_dictionary[label])\\n                draw.rectangle(xy=xy, outline=\\'green\\', width=5)\\n                \\n    \\n    #for label in train_dataset.real_labels[idx]:\\n    #    draw.rectangle(xy=label[:4], outline=\\'red\\', width=5)\\n    \\n    ax.imshow(image)\\n'"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["class TrafficSignImagesDataset(torch.utils.data.Dataset):\n","    def __init__(self, df, files_dir, transform=None):\n","        self.annotations = df\n","        self.files_dir = files_dir\n","        self.transform = transform\n","        self.S = Config.S\n","        self.B = Config.B\n","        self.C = Config.C\n","        \n","        self.cache = {}\n","        self.use_cache = True\n","        self.cache_hits = 0\n","        self.repetitions = 3\n","\n","    def clear_cache(self):\n","        print(\"Cleared {} cached entries (hits: {})\".format(len(self.cache), self.cache_hits))\n","        self.cache_hits = 0\n","        self.cache = {}\n","        \n","    def __len__(self):\n","        return len(self.annotations) * self.repetitions\n","    \n","    def __getitem__(self, index):\n","        if not self.use_cache:\n","            return self.load_index(index)\n","        \n","        if not index in self.cache:\n","            self.cache[index] = self.load_index(index)\n","        else:\n","            self.cache_hits += 1\n","        return self.cache[index]\n","\n","    def load_index(self, virtual_index):\n","        index = virtual_index % len(self.annotations)\n","        dataset_row = self.annotations.iloc[index]\n","        image_path = os.path.join(self.files_dir, 'images', dataset_row[0])\n","        label_path = os.path.join(self.files_dir, 'labels', dataset_row[1])\n","        \n","        boxes = []\n","        labels = []\n","        \n","        img = Image.open(image_path)\n","        img_width, img_height = img.size\n","        \n","        with open(label_path, 'r') as csvfile:\n","            labelreader = csv.reader(csvfile, delimiter=',')\n","            for row in labelreader:\n","                if len(row) == 5:\n","                    [xmin,ymin, xmax, ymax] = [int(x) for x in row[:4]]\n","                    \n","                    centerx = ((xmax + xmin) / 2) / img_width\n","                    centery = ((ymax + ymin) / 2) / img_height\n","                    boxwidth = (xmax - xmin) / img_width\n","                    boxheight = (ymax - ymin) / img_height\n","                    \n","                    if row[4] in class_dictionary:\n","                        class_num = class_dictionary.index(row[4])\n","                        boxes.append([centerx, centery, boxwidth, boxheight, class_num])\n","                    else:\n","                        continue\n","                \n","        #boxes = torch.tensor(boxes)\n","        image = img.convert(\"RGB\")\n","        image = np.array(image)\n","        \n","        if self.transform:\n","            transformed = self.transform(image=image, bboxes=boxes)\n","            image = transformed['image']\n","            boxes = transformed['bboxes']\n","        image = torch.tensor(image, dtype=torch.float32) / 255.0\n","            \n","        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B)) \n","        for box in boxes:\n","            x, y, width, height, class_label = box\n","            class_label = int(class_label)\n","\n","            # i,j represents the cell row and cell column\n","            i, j = int(self.S * y), int(self.S * x)\n","            x_cell, y_cell = self.S * x - j, self.S * y - i\n","\n","            width_cell, height_cell = ( width * self.S, height * self.S )\n","\n","            # If no object already found for specific cell i,j\n","            # Note: This means we restrict to ONE object\n","            # per cell!\n","            if label_matrix[i, j, self.C] == 0:\n","                # Set that there exists an object\n","                label_matrix[i, j, self.C] = 1\n","\n","                # Box coordinates\n","                box_coordinates = torch.tensor(\n","                    [x_cell, y_cell, width_cell, height_cell]\n","                )\n","\n","                label_matrix[i, j, 8:12] = box_coordinates\n","                \n","\n","                # Set one hot encoding for class_label\n","                label_matrix[i, j, class_label] = 1\n","\n","        return image, label_matrix\n","    \n","    \n","'''\n","\n","for ridx, ax in enumerate(axs.ravel()):\n","    idx = ridx + 560\n","    im = torch.moveaxis(train_dataset[idx][0], 0, 2).cpu().numpy()\n","    \n","    image = Image.fromarray((im * 255).astype(np.uint8))\n","    draw = ImageDraw.Draw(image)\n","    \n","    \n","    labels = train_dataset[idx][1]\n","    S_SIZE = im.shape[1] / Config.S\n","    \n","    for i in range(Config.S):\n","        for j in range(Config.S):\n","            if labels[i, j, Config.C] == 1.0:\n","                [x, y, lwidth, lheight] = labels[i, j, 8:12] * S_SIZE\n","                lwidth /= 2\n","                lheight /= 2\n","                \n","                x += j * S_SIZE\n","                y += i * S_SIZE\n","                \n","                xy = [x - lwidth, y - lheight, x + lwidth, y + lheight]\n","        \n","                label = int( np.argmax(labels[i, j, :7]))\n","                print(\"label\", class_dictionary[label])\n","                draw.rectangle(xy=xy, outline='green', width=5)\n","                \n","    \n","    #for label in train_dataset.real_labels[idx]:\n","    #    draw.rectangle(xy=label[:4], outline='red', width=5)\n","    \n","    ax.imshow(image)\n","'''"]},{"cell_type":"markdown","metadata":{},"source":["# **Model Loss**"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T17:58:50.812390Z","iopub.status.busy":"2024-06-23T17:58:50.812023Z","iopub.status.idle":"2024-06-23T17:58:50.832379Z","shell.execute_reply":"2024-06-23T17:58:50.831279Z","shell.execute_reply.started":"2024-06-23T17:58:50.812363Z"},"trusted":true},"outputs":[],"source":["class YoloLoss(nn.Module):\n","\n","    def __init__(self):\n","        super(YoloLoss, self).__init__()\n","        self.mse = nn.MSELoss(reduction=\"sum\")\n","        \n","        self.S = Config.S\n","        self.B = Config.B\n","        self.C = Config.C\n","\n","        self.lambda_noobj = 0.5\n","        self.lambda_coord = 5\n","\n","    def forward(self, predictions, target):\n","        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted\n","        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n","\n","        # Calculate IoU for the two predicted bounding boxes with target bbox\n","        iou_b1 = intersection_over_union(predictions[..., self.C + 1:self.C + 5], target[..., self.C + 1:self.C + 5])\n","        iou_b2 = intersection_over_union(predictions[..., self.C + 6:self.C + 10], target[..., self.C + 1:self.C + 5])\n","        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n","\n","        # Take the box with highest IoU out of the two prediction\n","        # Note that bestbox will be indices of 0, 1 for which bbox was best\n","        iou_maxes, bestbox = torch.max(ious, dim=0)\n","        exists_box = target[..., self.C].unsqueeze(3)  # in paper this is Iobj_i\n","\n","        # ======================== #\n","        #   FOR BOX COORDINATES    #\n","        # ======================== #\n","\n","        # Set boxes with no object in them to 0. We only take out one of the two \n","        # predictions, which is the one with highest Iou calculated previously.\n","        box_predictions = exists_box * (\n","            (\n","                bestbox * predictions[..., self.C + 6:self.C + 10]\n","                + (1 - bestbox) * predictions[..., self.C + 1:self.C + 5]\n","            )\n","        )\n","\n","        box_targets = exists_box * target[..., self.C + 1:self.C + 5]\n","\n","        # Take sqrt of width, height of boxes to ensure that\n","        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n","            torch.abs(box_predictions[..., 2:4] + 1e-6)\n","        )\n","        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n","\n","        box_loss = self.mse(\n","            torch.flatten(box_predictions, end_dim=-2),\n","            torch.flatten(box_targets, end_dim=-2),\n","        )\n","\n","        # ==================== #\n","        #   FOR OBJECT LOSS    #\n","        # ==================== #\n","\n","        # pred_box is the confidence score for the bbox with highest IoU\n","        pred_box = (\n","            bestbox * predictions[..., self.C + 5:self.C + 6] + (1 - bestbox) * predictions[..., self.C:self.C + 1]\n","        )\n","\n","        object_loss = self.mse(\n","            torch.flatten(exists_box * pred_box),\n","            torch.flatten(exists_box * target[..., self.C:self.C + 1]),\n","        )\n","\n","        # ======================= #\n","        #   FOR NO OBJECT LOSS    #\n","        # ======================= #\n","\n","        #max_no_obj = torch.max(predictions[..., 20:21], predictions[..., 25:26])\n","        #no_object_loss = self.mse(\n","        #    torch.flatten((1 - exists_box) * max_no_obj, start_dim=1),\n","        #    torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n","        #)\n","\n","        no_object_loss = self.mse(\n","            torch.flatten((1 - exists_box) * predictions[..., self.C:self.C + 1], start_dim=1),\n","            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1),\n","        )\n","\n","        no_object_loss += self.mse(\n","            torch.flatten((1 - exists_box) * predictions[..., self.C + 5:self.C + 6], start_dim=1),\n","            torch.flatten((1 - exists_box) * target[..., self.C:self.C + 1], start_dim=1)\n","        )\n","\n","        # ================== #\n","        #   FOR CLASS LOSS   #\n","        # ================== #\n","\n","        class_loss = self.mse(\n","            torch.flatten(exists_box * predictions[..., :self.C], end_dim=-2,),\n","            torch.flatten(exists_box * target[..., :self.C], end_dim=-2,),\n","        )\n","\n","        loss = (\n","            self.lambda_coord * box_loss  # first two rows in paper\n","            + object_loss  # third row in paper\n","            + self.lambda_noobj * no_object_loss  # forth row\n","            + class_loss  # fifth row\n","        )\n","\n","        return loss"]},{"cell_type":"markdown","metadata":{},"source":["# **Model Training**"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T20:20:55.046733Z","iopub.status.busy":"2024-06-23T20:20:55.046289Z","iopub.status.idle":"2024-06-23T20:20:55.053108Z","shell.execute_reply":"2024-06-23T20:20:55.052047Z","shell.execute_reply.started":"2024-06-23T20:20:55.046699Z"},"trusted":true},"outputs":[],"source":["LEARNING_RATE = 2e-5\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","BATCH_SIZE = 16\n","WEIGHT_DECAY = 0\n","EPOCHS = 100\n","LOAD_MODEL = False\n","LOAD_MODEL_FILE = \"model-large-7.3-576-4x.pth\"\n","TMP_MODEL_FILE = \"model-tmp.pth\""]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T17:58:52.473311Z","iopub.status.busy":"2024-06-23T17:58:52.473023Z","iopub.status.idle":"2024-06-23T17:58:52.483315Z","shell.execute_reply":"2024-06-23T17:58:52.482506Z","shell.execute_reply.started":"2024-06-23T17:58:52.473287Z"},"trusted":true},"outputs":[],"source":["def train_fn(train_loader, model, optimizer, loss_fn):\n","    loop = tqdm(train_loader, leave=True)\n","    mean_loss = []\n","    \n","    for batch_idx, (x, y) in enumerate(loop):\n","        x, y = x.to(DEVICE), y.to(DEVICE)\n","        out = model(x)\n","        loss = loss_fn(out, y)\n","        mean_loss.append(loss.item())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        loop.set_postfix(loss = loss.item())\n","        \n","    print(f\"Mean loss was {sum(mean_loss) / len(mean_loss)}\")\n","    return sum(mean_loss) / len(mean_loss)"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T17:58:52.485224Z","iopub.status.busy":"2024-06-23T17:58:52.484572Z","iopub.status.idle":"2024-06-23T17:58:52.493934Z","shell.execute_reply":"2024-06-23T17:58:52.493012Z","shell.execute_reply.started":"2024-06-23T17:58:52.485188Z"},"trusted":true},"outputs":[],"source":["def calc_map(data_loader, model):\n","    pred_boxes, target_boxes = get_bboxes(\n","        data_loader, model, iou_threshold=0.5, threshold=0.4\n","    )\n","\n","    mean_avg_prec = mean_average_precision(\n","        pred_boxes, target_boxes, iou_threshold=0.5, \n","        box_format=\"midpoint\"\n","    )\n","    \n","    \n","    return mean_avg_prec"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T17:58:52.495374Z","iopub.status.busy":"2024-06-23T17:58:52.495047Z","iopub.status.idle":"2024-06-23T19:32:57.587038Z","shell.execute_reply":"2024-06-23T19:32:57.585996Z","shell.execute_reply.started":"2024-06-23T17:58:52.495339Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["=> Loading checkpoint\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/132 [00:00<?, ?it/s]/tmp/ipykernel_619/2122474196.py:36: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  image_path = os.path.join(self.files_dir, 'images', dataset_row[0])\n","/tmp/ipykernel_619/2122474196.py:37: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  label_path = os.path.join(self.files_dir, 'labels', dataset_row[1])\n","/tmp/ipykernel_619/2122474196.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  image = torch.tensor(image, dtype=torch.float32) / 255.0\n","100%|██████████| 132/132 [01:01<00:00,  2.15it/s, loss=17]  \n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 33.172677632534146\n","Training data mAP: 0.6267604827880859\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.98it/s, loss=2.93]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 23.357967922181793\n","Training data mAP: 0.6929622888565063\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.97it/s, loss=10.9]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 19.97017453656052\n","Training data mAP: 0.7395100593566895\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.96it/s, loss=2.7] \n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 17.763590879512556\n","Training data mAP: 0.7728337645530701\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.97it/s, loss=1.82]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 16.33938034556129\n","Training data mAP: 0.8100749850273132\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.96it/s, loss=2.96]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 15.049295815554531\n","Training data mAP: 0.8255395889282227\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.97it/s, loss=9.94]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 14.099350055058798\n","Training data mAP: 0.8391776084899902\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.96it/s, loss=2.63]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 13.23802204023708\n","Training data mAP: 0.8596371412277222\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.97it/s, loss=1.4] \n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 12.538257639516484\n","Training data mAP: 0.8662630915641785\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=3.89]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 11.968930630972892\n","Training data mAP: 0.8911338448524475\n","Validation mAP: 0.28446826338768005\n","10 tensor(0,2845) 11,968930630972892\n","Saving tmp file\n","=> Saving checkpoint\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=8.89]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 11.456422368685404\n","Training data mAP: 0.9065121412277222\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.91it/s, loss=5.93]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 10.857257875529202\n","Training data mAP: 0.9033292531967163\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=2.75]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 10.432272519126084\n","Training data mAP: 0.915012776851654\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.94it/s, loss=2.97]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 10.051290488604343\n","Training data mAP: 0.92353355884552\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.93it/s, loss=2.07]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 9.625406915491277\n","Training data mAP: 0.9166001677513123\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.96it/s, loss=2.43]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 9.328946940826647\n","Training data mAP: 0.936197817325592\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.97it/s, loss=3.16]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 8.935835616155105\n","Training data mAP: 0.9433971047401428\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.96it/s, loss=3.34]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 8.84988889910958\n","Training data mAP: 0.9388279318809509\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.97it/s, loss=3.28]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 8.376352373397712\n","Training data mAP: 0.9409889578819275\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=3.88]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 8.234320463556232\n","Training data mAP: 0.9450309872627258\n","Validation mAP: 0.28968167304992676\n","20 tensor(0,2897) 8,234320463556232\n","Saving tmp file\n","=> Saving checkpoint\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.93it/s, loss=1.27]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 8.069273843909755\n","Training data mAP: 0.9483684301376343\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.97it/s, loss=3.92]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 7.7487638556596\n","Training data mAP: 0.9399523138999939\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.96it/s, loss=2.5] \n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 7.514674706892534\n","Training data mAP: 0.9442297220230103\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.97it/s, loss=2.13]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 7.340929826100667\n","Training data mAP: 0.9484343528747559\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.97it/s, loss=2.69]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 7.217334248802879\n","Training data mAP: 0.9412404298782349\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.97it/s, loss=1.33]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 7.00378713463292\n","Training data mAP: 0.9527220129966736\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.97it/s, loss=2.09]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 6.76416394927285\n","Training data mAP: 0.9610352516174316\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.97it/s, loss=7.38]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 6.675326394312309\n","Training data mAP: 0.9612723588943481\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.96it/s, loss=2.61]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 6.484269315546209\n","Training data mAP: 0.9636077880859375\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.97it/s, loss=2.23]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 6.448035552646175\n","Training data mAP: 0.9606267809867859\n","Validation mAP: 0.2246176153421402\n","30 tensor(0,2246) 6,448035552646175\n","Saving tmp file\n","=> Saving checkpoint\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=1.63]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 6.246390880960407\n","Training data mAP: 0.9534136056900024\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=1.52]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 6.038932919502258\n","Training data mAP: 0.9474702477455139\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.93it/s, loss=2.42]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 5.9536329489765745\n","Training data mAP: 0.9495641589164734\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.94it/s, loss=1.75]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 5.833123124007023\n","Training data mAP: 0.956572413444519\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.94it/s, loss=2.6] \n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 5.728970690207048\n","Training data mAP: 0.9709315896034241\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.93it/s, loss=1.39]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 5.632698414000598\n","Training data mAP: 0.9665804505348206\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=1.47]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 5.607647052316954\n","Training data mAP: 0.9598610997200012\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.93it/s, loss=1.26]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 5.449889429590919\n","Training data mAP: 0.9580075144767761\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=1.14]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 5.470724261168278\n","Training data mAP: 0.9538341164588928\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.93it/s, loss=1.53]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 5.327625408317104\n","Training data mAP: 0.9646824598312378\n","Validation mAP: 0.24432624876499176\n","40 tensor(0,2443) 5,327625408317104\n","Saving tmp file\n","=> Saving checkpoint\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.94it/s, loss=1.97]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 5.134750413172172\n","Training data mAP: 0.9744866490364075\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=3.57]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 5.172526258410829\n","Training data mAP: 0.9588943123817444\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.94it/s, loss=2.21]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 5.628469555666952\n","Training data mAP: 0.9427345991134644\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.93it/s, loss=1.12]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 5.574991367983095\n","Training data mAP: 0.9660683274269104\n","Epoch 00044: reducing learning rate of group 0 to 1.2500e-06.\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.94it/s, loss=2.33]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 4.905621234214667\n","Training data mAP: 0.9817270040512085\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.94it/s, loss=1.25]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 4.691975949388562\n","Training data mAP: 0.9844270348548889\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=3.82]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 4.634768719022924\n","Training data mAP: 0.9865558743476868\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.94it/s, loss=2.33]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 4.565675695737203\n","Training data mAP: 0.9811937212944031\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=1.5] \n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 4.4890476790341465\n","Training data mAP: 0.9842526316642761\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.94it/s, loss=2.32]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 4.4692605119763\n","Training data mAP: 0.9806360006332397\n","Validation mAP: 0.21734336018562317\n","50 tensor(0,2173) 4,4692605119763\n","Saving tmp file\n","=> Saving checkpoint\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=2.83]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 4.4590585574959265\n","Training data mAP: 0.9758864045143127\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.96it/s, loss=2.62]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 4.311225367314888\n","Training data mAP: 0.9807964563369751\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.96it/s, loss=2.47]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 4.257392270998522\n","Training data mAP: 0.9806380271911621\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=3.06]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 4.26825683405905\n","Training data mAP: 0.9860311150550842\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=2.39]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 4.262662602193428\n","Training data mAP: 0.9786384701728821\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.96it/s, loss=4.46]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 4.248703514084672\n","Training data mAP: 0.9816211462020874\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=1.28]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 4.148741760037162\n","Training data mAP: 0.9821364283561707\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=1.41]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 4.136509719220075\n","Training data mAP: 0.9781137704849243\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.94it/s, loss=1.29]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 4.055737137794495\n","Training data mAP: 0.980681836605072\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=2.13]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 4.006193233258797\n","Training data mAP: 0.9838514924049377\n","Validation mAP: 0.22656093537807465\n","60 tensor(0,2266) 4,006193233258797\n","Saving tmp file\n","=> Saving checkpoint\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.94it/s, loss=4.32]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 4.0803317608255325\n","Training data mAP: 0.9825819134712219\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.93it/s, loss=2.13]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.9740207556522256\n","Training data mAP: 0.9781715273857117\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=1.77]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.88758863102306\n","Training data mAP: 0.9872912764549255\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.93it/s, loss=1.23]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.8914437510750512\n","Training data mAP: 0.981358528137207\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.93it/s, loss=0.803]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.7947290472008963\n","Training data mAP: 0.9777730107307434\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.93it/s, loss=2.12]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.8799536426862082\n","Training data mAP: 0.9868890047073364\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.93it/s, loss=2.28]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.7704467032894944\n","Training data mAP: 0.9851322174072266\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.94it/s, loss=0.895]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.7308621605237327\n","Training data mAP: 0.9810949563980103\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.94it/s, loss=1.81]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.7457990781827406\n","Training data mAP: 0.981486976146698\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.92it/s, loss=1.95]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.7217285497622057\n","Training data mAP: 0.9806021451950073\n","Validation mAP: 0.22555139660835266\n","70 tensor(0,2256) 3,7217285497622057\n","Saving tmp file\n","=> Saving checkpoint\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.94it/s, loss=1.16]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.7434026264783107\n","Training data mAP: 0.9869184494018555\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.94it/s, loss=3.35]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.7020203733082973\n","Training data mAP: 0.983610987663269\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.94it/s, loss=1.68]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.695634151950027\n","Training data mAP: 0.9823024868965149\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=2.74]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.5893570690444023\n","Training data mAP: 0.9816433191299438\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.93it/s, loss=1.45]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.591013156103365\n","Training data mAP: 0.9795967936515808\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=1.36]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.570087803132606\n","Training data mAP: 0.9858319163322449\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.98it/s, loss=2.41]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.5949751788919624\n","Training data mAP: 0.9895927309989929\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=1.33]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.523792696721626\n","Training data mAP: 0.9867472052574158\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.96it/s, loss=1.25]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.4987469550335044\n","Training data mAP: 0.977078378200531\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.94it/s, loss=1.15]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.541467263843074\n","Training data mAP: 0.9840460419654846\n","Validation mAP: 0.2239328771829605\n","80 tensor(0,2239) 3,541467263843074\n","Saving tmp file\n","=> Saving checkpoint\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.94it/s, loss=1.17]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.41714989326217\n","Training data mAP: 0.984122097492218\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=0.889]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.399669640895092\n","Training data mAP: 0.9870612025260925\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.94it/s, loss=0.83]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.4212664636698635\n","Training data mAP: 0.987418532371521\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.93it/s, loss=1.67]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.4042405504168887\n","Training data mAP: 0.9864639639854431\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.92it/s, loss=3.94]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.421518378185503\n","Training data mAP: 0.9809283018112183\n","Epoch 00085: reducing learning rate of group 0 to 6.2500e-07.\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=0.991]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.2917303000435685\n","Training data mAP: 0.9886088371276855\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.96it/s, loss=2.9] \n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.2436910578698823\n","Training data mAP: 0.9835720062255859\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=1.1] \n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.1430421327099656\n","Training data mAP: 0.9881632924079895\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.93it/s, loss=2.24]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.2024785352475718\n","Training data mAP: 0.9850059151649475\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.96it/s, loss=1.63]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.059813346826669\n","Training data mAP: 0.9920949935913086\n","Validation mAP: 0.23132732510566711\n","90 tensor(0,2313) 3,059813346826669\n","Saving tmp file\n","=> Saving checkpoint\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.94it/s, loss=0.918]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.1363345673589995\n","Training data mAP: 0.9896483421325684\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.96it/s, loss=1.05]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.045069436232249\n","Training data mAP: 0.9894833564758301\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.94it/s, loss=5.62]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.1356243843382057\n","Training data mAP: 0.9887120127677917\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.93it/s, loss=0.928]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.0929408208890394\n","Training data mAP: 0.9880391359329224\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.95it/s, loss=1.05]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.091881803490899\n","Training data mAP: 0.9885891675949097\n","Epoch 00095: reducing learning rate of group 0 to 3.1250e-07.\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.97it/s, loss=1.05]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.000576329050642\n","Training data mAP: 0.9922992587089539\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.97it/s, loss=0.865]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 3.0249083358230013\n","Training data mAP: 0.9893946647644043\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.97it/s, loss=1.36]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 2.9892692493669912\n","Training data mAP: 0.9909056425094604\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.97it/s, loss=1.97]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 2.941788133346673\n","Training data mAP: 0.9939131140708923\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 132/132 [00:26<00:00,  4.98it/s, loss=1.37]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 2.96002814896179\n","Training data mAP: 0.9902182221412659\n","Validation mAP: 0.22721931338310242\n","100 tensor(0,2272) 2,96002814896179\n","Saving tmp file\n","=> Saving checkpoint\n","=> Saving checkpoint\n"]}],"source":["def main():\n","    model = YoloV1().to(DEVICE)\n","\n","    optimizer = optim.Adam(\n","        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n","    )\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.5, patience=2, mode='min', verbose=True)\n","    loss_fn = YoloLoss()\n","\n","    if LOAD_MODEL:\n","        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n","\n","    train_dataset = TrafficSignImagesDataset( df=df, transform=new_transform, files_dir=files_dir )\n","\n","    val_dataset = TrafficSignImagesDataset(\n","        df=val_df,\n","        transform=new_transform_val, \n","        files_dir=val_dir\n","    )\n","\n","    train_loader = DataLoader(\n","        dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False\n","    )\n","\n","    val_loader = DataLoader(\n","        dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False,drop_last=False,\n","    )\n","\n","    for epoch in range(EPOCHS):\n","        loss = train_fn(train_loader, model, optimizer, loss_fn)\n","        \n","        mean_avg_prec_train = calc_map(train_loader, model)\n","        print(f\"Training data mAP: {mean_avg_prec_train}\")\n","        \n","        scheduler.step(loss)\n","        \n","        if (epoch + 1) % 10 == 0:\n","            #train_dataset.clear_cache()\n","            mean_avg_prec = calc_map(val_loader, model)\n","            print(f\"Validation mAP: {mean_avg_prec}\")\n","        \n","            print(epoch+1, str(mean_avg_prec).replace(\".\",\",\"), str(loss).replace(\".\",\",\"))\n","            print(\"Saving tmp file\")\n","            checkpoint = {\n","                \"state_dict\": model.state_dict(),\n","                \"optimizer\": optimizer.state_dict(),\n","            }\n","            save_checkpoint(checkpoint, filename=TMP_MODEL_FILE)\n","    \n","    checkpoint = {\n","            \"state_dict\": model.state_dict(),\n","            \"optimizer\": optimizer.state_dict(),\n","    }\n","    save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)\n","    \n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"markdown","metadata":{},"source":["# **Predictions**"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-06-23T19:51:41.225528Z","iopub.status.busy":"2024-06-23T19:51:41.224588Z","iopub.status.idle":"2024-06-23T19:51:50.735828Z","shell.execute_reply":"2024-06-23T19:51:50.734799Z","shell.execute_reply.started":"2024-06-23T19:51:41.225484Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["=> Loading checkpoint\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_619/2122474196.py:36: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  image_path = os.path.join(self.files_dir, 'images', dataset_row[0])\n","/tmp/ipykernel_619/2122474196.py:37: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  label_path = os.path.join(self.files_dir, 'labels', dataset_row[1])\n","/tmp/ipykernel_619/2122474196.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  image = torch.tensor(image, dtype=torch.float32) / 255.0\n","100%|██████████| 17/17 [00:05<00:00,  2.98it/s, loss=10.2]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss was 39.140269335578466\n","Test mAP: 0.6176915764808655\n"]}],"source":["PRED_BOXES = []\n","\n","def predictions():\n","    global PRED_BOXES\n","    model = YoloV1().to(DEVICE)\n","    \n","    optimizer = optim.Adam(\n","        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n","    )\n","    loss_fn = YoloLoss()\n","\n","    load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n","    \n","    val_dataset = TrafficSignImagesDataset(\n","        transform=new_transform_val, \n","        #df = df,\n","        #files_dir=files_dir\n","        df=val_df,\n","        files_dir=val_dir\n","    )\n","\n","    val_loader = DataLoader(\n","        dataset=val_dataset,\n","        batch_size=BATCH_SIZE,\n","        shuffle=False,\n","        drop_last=False,\n","    )\n","        \n","    model.eval()\n","    train_fn(val_loader, model, optimizer, loss_fn)\n","\n","    pred_boxes, target_boxes, x_values = get_bboxes(\n","        val_loader, model, iou_threshold=0.5, threshold=0.4, output_x = True,\n","    )\n","    PRED_BOXES = [pred_boxes, x_values]\n","\n","    mean_avg_prec = mean_average_precision(\n","        pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n","    )\n","    print(f\"Test mAP: {mean_avg_prec}\")\n","\n","\n","predictions()"]},{"cell_type":"markdown","metadata":{},"source":["# Make predictions and write them to file (validation)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-23T19:32:57.629453Z","iopub.status.idle":"2024-06-23T19:32:57.629940Z","shell.execute_reply":"2024-06-23T19:32:57.629701Z","shell.execute_reply.started":"2024-06-23T19:32:57.629681Z"},"trusted":true},"outputs":[],"source":["PRED_BOXES = []\n","\n","def get_pred_bboxes(loader, model, iou_threshold, threshold, pred_format=\"cells\", \n","               box_format=\"midpoint\", device=\"cuda\"):\n","    all_pred_boxes = []\n","    all_true_boxes = []\n","\n","    model.eval()\n","    train_idx = 0\n","\n","    for batch_idx, (x,y) in enumerate(loader):\n","        x = x.to(device)\n","        [file_names, widths, heights] = y\n","        #labels = labels.to(device)\n","\n","        with torch.no_grad():\n","            predictions = model(x)\n","\n","        batch_size = x.shape[0]\n","        #true_bboxes = cellboxes_to_boxes(labels)\n","        bboxes = cellboxes_to_boxes(predictions)\n","\n","        for idx in range(batch_size):\n","            \n","            width = widths[idx].item()\n","            height = heights[idx].item()\n","            \n","            nms_boxes = non_max_suppression(\n","                bboxes[idx],\n","                iou_threshold=iou_threshold,\n","                threshold=threshold,\n","                box_format=box_format,\n","            )\n","\n","            for nms_box in nms_boxes:\n","                [class_id, prob, cx, cy, w, h] = nms_box\n","                \n","                cx *= width\n","                cy *= height\n","\n","                w *= width / 2.0\n","                h *= height / 2.0\n","\n","                x1 = int(cx - w)\n","                x2 = int(cx + w)\n","\n","                y1 = int(cy - h)\n","                y2 = int(cy + h)\n","                \n","                all_pred_boxes.append([\n","                    file_names[idx].replace('.jpg', ''),\n","                    x1, y1,\n","                    x2, y2,\n","                    prob,\n","                    class_dictionary[int(class_id)]\n","                ])\n","                #all_pred_boxes.append([train_idx, file_names[idx], width, height] + nms_box)\n","\n","            #for box in true_bboxes[idx]:\n","            #    if box[1] > threshold:\n","            #        all_true_boxes.append([train_idx] + box)\n","\n","            train_idx += 1\n","\n","    model.train()\n","    return all_pred_boxes\n","\n","\n","def prediction_test_dataset():\n","    model = YoloV1().to(DEVICE)\n","    \n","    optimizer = optim.Adam(\n","        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n","    )\n","    loss_fn = YoloLoss()\n","\n","    load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n","    \n","    test_dataset = TrafficSignTestImagesDataset(\n","        transform=new_transform_val, \n","        df=test_df,\n","        files_dir=test_dir\n","    )\n","\n","    test_loader = DataLoader(\n","        dataset=test_dataset,\n","        batch_size=BATCH_SIZE,\n","        shuffle=False,\n","        drop_last=False,\n","    )\n","        \n","    model.eval()\n","\n","    pred_boxes = get_pred_bboxes(\n","        test_loader, model, iou_threshold=0.5, threshold=0.4, output_x = False,\n","    )\n","    \n","    with open('predictions.csv', 'w', newline='\\n') as f:\n","        writer = csv.writer(f)\n","        writer.writerows(pred_boxes)\n","\n","\n","prediction_test_dataset()"]},{"cell_type":"markdown","metadata":{},"source":["# Predictions (test dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def draw_image(x, y, y_pred=None):\n","    img = x.cpu() \n","    X = img.shape[1]\n","    \n","    labels = y.cpu().numpy()\n","\n","    img_np = np.moveaxis((img.numpy() * 255.0).astype(np.uint8),0,2)\n","    img = Image.fromarray(img_np)\n","    \n","    [width, height,_] = img_np.shape\n","    \n","    draw = ImageDraw.Draw(img)\n","\n","    S = Config.S\n","    \n","    S_SIZE = X / S\n","    \n","    if y_pred is not None:\n","        for [L, C, x, y, lwidth, lheight] in y_pred:\n","            print(\"FOUND\", class_dictionary[int(L)], \"p=\", C, x, y, lwidth, lheight)\n","            x *= width\n","            y *= height\n","            \n","            lwidth *= width / 2.0\n","            lheight *= height / 2.0\n","            \n","            x1 = int(x - lwidth)\n","            x2 = int(x + lwidth)\n","\n","            y1 = int(y - lheight)\n","            y2 = int(y + lheight)\n","            try:\n","                draw.rectangle(xy=[x1,y1,x2,y2], outline='red', width=5)\n","            except:\n","                pass\n","                #print(x1,y1,x2,y2)\n","            \n","    for i in range(Config.S):\n","        for j in range(Config.S):\n","            if labels[i, j, Config.S] == 1.0:\n","                [x, y, lwidth, lheight] = labels[i, j, 8:12] * S_SIZE\n","                \n","                \n","                lwidth /= 2\n","                lheight /= 2\n","                \n","                x += j * S_SIZE\n","                y += i * S_SIZE\n","                \n","                xy = [x - lwidth, y - lheight, x + lwidth, y + lheight]\n","                label = int( np.argmax(labels[i, j, :7]))\n","                print(\"label\", class_dictionary[label], \"\", labels[i, j, :7])\n","                draw.rectangle(xy=xy, outline='green', width=5)\n","                \n","    print(\"*******\")\n","                \n","    \n","    return np.array(img)\n","\n","fig, axs = plt.subplots(2, 3, figsize=(15, 6))\n","\n","XY = PRED_BOXES[1]\n","\n","for idx, ax in enumerate(axs.ravel()):\n","    IDX = idx + 6 * 14\n","    \n","    y_preds = []\n","    \n","    idx_id, x, y = None, None, None\n","    \n","    for row in XY:\n","        if row[0] == IDX:\n","            if len(y_preds) == 0:\n","                [idx_id, x, y, y_pred] = row\n","            \n","            y_preds.append(row[3])\n","    \n","    if x is None or y is None:\n","        continue\n","    img = draw_image(x, y, y_preds)\n","    \n","    ax.imshow(img)\n","    ax.axis('off')"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":8922151,"datasetId":5095129,"sourceId":8766578,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
